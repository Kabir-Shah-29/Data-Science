{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b789badf",
   "metadata": {},
   "source": [
    "# DEEP LEARNING THEORY PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is deep learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53332bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning is a branch of machine learning\n",
    "# The deep learning uses artificial neurons network to understand the context of images, natural \n",
    "# language and speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e17d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep learning :-\n",
    "    artificial neurons:- biological neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1155d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input(weight& strength)->Sum->Transform->Output\n",
    "# Y=MX\n",
    "# X1(W1)+X2(W2)+X3(W3)=OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e262a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN:-Artificial neural network has been inspired by biological neural network ! \n",
    "# A grain size pea's piece of human \n",
    "#  brain contains 10,000 neurons, each of the neurons form an average of 6000 connection with \n",
    "# the other neurons.\n",
    "\n",
    "# The neurons = The artifical neurons also take some no. of inputs x1,x2,x3 each of the input is \n",
    "# multiplied by specific\n",
    "# weight w1,w2,....wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow:-TensorFlow is an open-source machine learning framework developed by Google. \n",
    "# It provides a comprehensive set of tools and libraries for building, training, and deploying a wide\n",
    "# range of machine learning models, including deep learning models. TensorFlow is designed to be \n",
    "# flexible, scalable, and efficient, making it suitable for both research and production-level.\n",
    "# applications.\n",
    "# Tensorflow has two version:-\n",
    "# 1X:-1. session:- \n",
    "#     2. Lazy execution\n",
    "#     3. need to initialize variable\n",
    "#     4. Placeholder:- a \n",
    "# 2X:- 1. print\n",
    "#      2. Eager exe\n",
    "#      3. not need to initialize variable\n",
    "#      4. Keras:- api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5697be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "1x\n",
    "2x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d585d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result= [5, 2, 3, 10, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "# 1.X:-\n",
    "a= tf.constant(5)\n",
    "b=tf.constant(2)\n",
    "c=tf.constant(3)\n",
    "e=b+c #5\n",
    "d=b*a #10\n",
    "f=d-e #5\n",
    "sess=tf.Session()\n",
    "# sess.run(f)\n",
    "fetches=[a,b,c,d,e,f]\n",
    "result=sess.run(fetches)\n",
    "sess.close()\n",
    "print(\"result=\",result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eec7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c94266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras:- Keras has become the official high-level API for defining and training neural networks \n",
    "# within TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bb39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168a92f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de2a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 5\n",
      "5 10 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define TensorFlow constants\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "\n",
    "# Perform operations directly without a session\n",
    "e = b + c\n",
    "d = b * a\n",
    "f = d - e\n",
    "\n",
    "# You can evaluate the result directly using .numpy()\n",
    "result = f.numpy()\n",
    "\n",
    "# Print the result\n",
    "print(\"result =\", result)\n",
    "print(e.numpy(),d.numpy(),f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate function:- It basically decide to activate or deactivate neurons to get desired output.\n",
    "# IT also performs non linearity on input to get better result on a complex neural network\n",
    "\n",
    "# types of activation function:-\n",
    "# 1. Sigmoid function:-range:- 0 to 1\n",
    "# shape:- S curve , which passes from origin\n",
    "# used for binary classification:- \n",
    "# F(Z)=1/1+e**-(W**Tx+b)\n",
    "\n",
    "# 2. Tanh Activation function :- range=-1 to 1\n",
    "# shape =S curve\n",
    "# used in RNN= text classification\n",
    "\n",
    "# 3. Relu Function:- range (0,infinite)\n",
    "# straight line shape\n",
    "# used in CNN=image classification/image recognition\n",
    "\n",
    "# 4. Softmax function:-it is used for multi class classification:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers in feed forwarded:-\n",
    "# Input layer:-This layer is used to feed the data.\n",
    "# no activation is applied to the input layer.\n",
    "# The no. of neurons in input layer should be equal to the no.of features .\n",
    "# output layer:-this layer is used for prediction.\n",
    "# based on the nature of the problem , we decide the no. of neurons in the output layer.\n",
    "# for regression , we need to predict single value hence we require only 1 neuron in the network.\n",
    "# for binary classification , we need 1 neurons in the output layer (sigmoid,crossentropy).\n",
    "# for binary classification , we need 2 neurons in the output layer (softmax,categorical_crossentropy).\n",
    "# for multi class classification with 5 classes , we need 5 neurons in the output layer(softmax ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25327761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need of deep learning\n",
    "# amount of data \n",
    "# hardware&software advancements\n",
    "# python & opensource ecosystem\n",
    "# framework like tensorflow & python\n",
    "# cloud &AI boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network (FNN):\n",
    "# A feedforward neural network, also known as a multilayer perceptron (MLP), consists of an input layer, one or more hidden\n",
    "# layers, and an output layer. It's called \"feedforward\" because the data flows in one direction, from the input layer through the \n",
    "# hidden layers to the output layer.\n",
    "# FNNs are capable of learning non-linear relationships in data due to the presence of hidden layers.\n",
    "# The depth of the network depends on the number of hidden layers, but it's not specifically categorized as shallow or deep.\n",
    "\n",
    "# Shallow Feedforward Neural Network:\n",
    "# A shallow feedforward neural network typically refers to an FNN with a small number of hidden layers or, in some contexts, just \n",
    "# one hidden layer.Shallow FNNs are suitable for relatively simple tasks where the data does not require a deep hierarchy of \n",
    "# feature transformations.They are easier to train and have fewer parameters, making them more suitable for cases with limited\n",
    "# data or computational resources.In the context of deep learning, \"shallow\" usually implies a network with only one or two \n",
    "# hidden layers.\n",
    "\n",
    "# Deep Feedforward Neural Network (Deep FNN):\n",
    "# A deep feedforward neural network has a significant number of hidden layers, typically more than two.\n",
    "# Deep FNNs are designed to handle complex data with intricate hierarchical structures and dependencies.\n",
    "# They excel in tasks where features at different levels of abstraction need to be learned automatically.\n",
    "# Deep learning, as a field, focuses on the study of deep FNNs and their variations.\n",
    "# Common deep FNN architectures include deep convolutional neural networks (CNNs) for image processing and deep recurrent neural\n",
    "# networks (RNNs) for sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss function:- we use loss function to measure the performance of a deep learning model for given data\n",
    "# # loss function is generally based on error terms.(actual-predicted)**2\n",
    "\n",
    "# # we use entropy for the classification\n",
    "# # Cross-Entropy Loss (Log Loss):\n",
    "\n",
    "# # Cross-Entropy Loss, often referred to simply as \"cross-entropy\" or \"log loss,\" is a widely used loss function for binary \n",
    "# # classification problems.\n",
    "# # It measures the dissimilarity between the predicted probability distribution (typically generated by a sigmoid activation \n",
    "# # function) and the true binary labels (0 or 1).\n",
    "\n",
    "# # Categorical Cross-Entropy Loss:\n",
    "# # Categorical Cross-Entropy Loss is used for multi-class classification problems, where the target variable can belong to one of \n",
    "# # multiple classes.\n",
    "# # It measures the dissimilarity between the predicted class probabilities (typically generated by a softmax activation function) \n",
    "# # and the true one-hot encoded class labels\n",
    "\n",
    "# if data is not encoded:- sparse categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23776df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is a hyperparameter used in many machine learning and optimization algorithms that determines the step size at \n",
    "# which a model's parameters are updated during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc661ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation is a supervised machine learning technique used to train neural networks. It involves adjusting the network's\n",
    "# internal parameters (weights and biases) based on the difference between predicted and actual outputs, moving backward through\n",
    "# the network to minimize errors and improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sequential model in the context of neural networks refers to a specific type of neural network architecture where the \n",
    "# layers are stacked sequentially, one on top of the other, to create a linear feedforward network. Each layer in a sequential\n",
    "# model passes its output to the next layer in a single direction, without any loops or complex connections between layers. \n",
    "# This architecture is sometimes called a \"feedforward neural network.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network for regression in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers are algorithms or methods used in machine learning and deep learning to adjust the \n",
    "# parameters of a model during \n",
    "# training in order to minimize a specific loss or error function. The primary goal of an optimizer \n",
    "# is to find the optimal set of model parameters that result in the best possible performance on a \n",
    "# given task. Optimizers play a crucial role in training neural networks and other machine learning\n",
    "# models by iteratively updating the model's weights and biases. \n",
    "\n",
    "# Adagrad: Adagrad is an adaptive optimization algorithm that adjusts the learning rates for each \n",
    "# model parameter based on the  historical gradient information, making it well-suited for sparse data.\n",
    "\n",
    "# RMSprop: RMSprop is an adaptive optimization algorithm that adapts learning rates by considering the\n",
    "# magnitude of recent gradients, addressing the vanishing/exploding gradient problem in training deep \n",
    "# neural networks.\n",
    "\n",
    "# Adam (Adaptive Moment Estimation): Adam is an adaptive optimization algorithm that combines the\n",
    "# benefits of momentum and RMSprop by adapting learning rates for each parameter based on historical \n",
    "# gradients and squared gradients, making it effective\n",
    "# in a wide range of machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579e835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 2s 170ms/step - loss: 0.5023 - mae: 0.6295 - val_loss: 0.1978 - val_mae: 0.3655\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1650 - mae: 0.3346 - val_loss: 0.1575 - val_mae: 0.3019\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1489 - mae: 0.3147 - val_loss: 0.2496 - val_mae: 0.4069\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2004 - mae: 0.3576 - val_loss: 0.2496 - val_mae: 0.4079\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.1801 - mae: 0.3375 - val_loss: 0.1856 - val_mae: 0.3337\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1323 - mae: 0.2952 - val_loss: 0.1342 - val_mae: 0.2710\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1122 - mae: 0.2829 - val_loss: 0.1240 - val_mae: 0.2744\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1193 - mae: 0.2907 - val_loss: 0.1260 - val_mae: 0.2818\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1253 - mae: 0.2963 - val_loss: 0.1216 - val_mae: 0.2739\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1139 - mae: 0.2844 - val_loss: 0.1160 - val_mae: 0.2563\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1020 - mae: 0.2719 - val_loss: 0.1223 - val_mae: 0.2695\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.1000 - mae: 0.2675 - val_loss: 0.1298 - val_mae: 0.2811\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.1025 - mae: 0.2704 - val_loss: 0.1295 - val_mae: 0.2822\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.1009 - mae: 0.2684 - val_loss: 0.1234 - val_mae: 0.2756\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0977 - mae: 0.2652 - val_loss: 0.1152 - val_mae: 0.2670\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0943 - mae: 0.2612 - val_loss: 0.1106 - val_mae: 0.2602\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0938 - mae: 0.2615 - val_loss: 0.1079 - val_mae: 0.2566\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0942 - mae: 0.2618 - val_loss: 0.1057 - val_mae: 0.2548\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0933 - mae: 0.2603 - val_loss: 0.1047 - val_mae: 0.2551\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0922 - mae: 0.2590 - val_loss: 0.1048 - val_mae: 0.2571\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0909 - mae: 0.2574 - val_loss: 0.1042 - val_mae: 0.2580\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0902 - mae: 0.2569 - val_loss: 0.1043 - val_mae: 0.2597\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0903 - mae: 0.2572 - val_loss: 0.1049 - val_mae: 0.2614\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0898 - mae: 0.2562 - val_loss: 0.1027 - val_mae: 0.2583\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0887 - mae: 0.2549 - val_loss: 0.1003 - val_mae: 0.2548\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0891 - mae: 0.2552 - val_loss: 0.0986 - val_mae: 0.2529\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0885 - mae: 0.2554 - val_loss: 0.0980 - val_mae: 0.2530\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0875 - mae: 0.2542 - val_loss: 0.0991 - val_mae: 0.2555\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0870 - mae: 0.2543 - val_loss: 0.1006 - val_mae: 0.2579\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0870 - mae: 0.2534 - val_loss: 0.0998 - val_mae: 0.2574\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0862 - mae: 0.2522 - val_loss: 0.0980 - val_mae: 0.2559\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0860 - mae: 0.2527 - val_loss: 0.0968 - val_mae: 0.2542\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0857 - mae: 0.2526 - val_loss: 0.0963 - val_mae: 0.2532\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0863 - mae: 0.2535 - val_loss: 0.0970 - val_mae: 0.2550\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0845 - mae: 0.2504 - val_loss: 0.0958 - val_mae: 0.2538\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0846 - mae: 0.2517 - val_loss: 0.0945 - val_mae: 0.2526\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0843 - mae: 0.2508 - val_loss: 0.0942 - val_mae: 0.2526\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0836 - mae: 0.2495 - val_loss: 0.0941 - val_mae: 0.2527\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0834 - mae: 0.2492 - val_loss: 0.0934 - val_mae: 0.2525\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0837 - mae: 0.2492 - val_loss: 0.0942 - val_mae: 0.2536\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0836 - mae: 0.2495 - val_loss: 0.0930 - val_mae: 0.2523\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0825 - mae: 0.2481 - val_loss: 0.0931 - val_mae: 0.2526\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0819 - mae: 0.2470 - val_loss: 0.0937 - val_mae: 0.2527\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0830 - mae: 0.2499 - val_loss: 0.0941 - val_mae: 0.2530\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0823 - mae: 0.2479 - val_loss: 0.0970 - val_mae: 0.2560\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0820 - mae: 0.2470 - val_loss: 0.0949 - val_mae: 0.2539\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0811 - mae: 0.2462 - val_loss: 0.0932 - val_mae: 0.2524\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0802 - mae: 0.2451 - val_loss: 0.0930 - val_mae: 0.2518\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0822 - mae: 0.2481 - val_loss: 0.0925 - val_mae: 0.2509\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0803 - mae: 0.2451 - val_loss: 0.0935 - val_mae: 0.2513\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0798 - mae: 0.2451 - val_loss: 0.0938 - val_mae: 0.2520\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0796 - mae: 0.2449 - val_loss: 0.0929 - val_mae: 0.2513\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0800 - mae: 0.2453 - val_loss: 0.0920 - val_mae: 0.2511\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0798 - mae: 0.2453 - val_loss: 0.0923 - val_mae: 0.2514\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0789 - mae: 0.2440 - val_loss: 0.0920 - val_mae: 0.2517\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0787 - mae: 0.2441 - val_loss: 0.0917 - val_mae: 0.2514\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0789 - mae: 0.2439 - val_loss: 0.0912 - val_mae: 0.2514\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0779 - mae: 0.2429 - val_loss: 0.0918 - val_mae: 0.2510\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0780 - mae: 0.2429 - val_loss: 0.0932 - val_mae: 0.2526\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0784 - mae: 0.2429 - val_loss: 0.0931 - val_mae: 0.2523\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0781 - mae: 0.2424 - val_loss: 0.0928 - val_mae: 0.2516\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0773 - mae: 0.2413 - val_loss: 0.0932 - val_mae: 0.2516\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.0774 - mae: 0.2414 - val_loss: 0.0930 - val_mae: 0.2518\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0768 - mae: 0.2406 - val_loss: 0.0936 - val_mae: 0.2520\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0771 - mae: 0.2411 - val_loss: 0.0936 - val_mae: 0.2525\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0765 - mae: 0.2400 - val_loss: 0.0938 - val_mae: 0.2525\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0763 - mae: 0.2404 - val_loss: 0.0932 - val_mae: 0.2520\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0759 - mae: 0.2400 - val_loss: 0.0929 - val_mae: 0.2514\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0758 - mae: 0.2397 - val_loss: 0.0921 - val_mae: 0.2503\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0756 - mae: 0.2398 - val_loss: 0.0915 - val_mae: 0.2512\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0755 - mae: 0.2403 - val_loss: 0.0914 - val_mae: 0.2505\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0752 - mae: 0.2396 - val_loss: 0.0921 - val_mae: 0.2502\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0749 - mae: 0.2388 - val_loss: 0.0924 - val_mae: 0.2493\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0749 - mae: 0.2383 - val_loss: 0.0930 - val_mae: 0.2487\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0748 - mae: 0.2376 - val_loss: 0.0930 - val_mae: 0.2499\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0747 - mae: 0.2380 - val_loss: 0.0927 - val_mae: 0.2529\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0742 - mae: 0.2374 - val_loss: 0.0924 - val_mae: 0.2519\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0738 - mae: 0.2363 - val_loss: 0.0930 - val_mae: 0.2516\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0746 - mae: 0.2365 - val_loss: 0.0931 - val_mae: 0.2527\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0738 - mae: 0.2357 - val_loss: 0.0915 - val_mae: 0.2531\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0741 - mae: 0.2376 - val_loss: 0.0910 - val_mae: 0.2548\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0736 - mae: 0.2368 - val_loss: 0.0910 - val_mae: 0.2532\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0748 - mae: 0.2372 - val_loss: 0.0924 - val_mae: 0.2530\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0737 - mae: 0.2345 - val_loss: 0.0918 - val_mae: 0.2525\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0738 - mae: 0.2361 - val_loss: 0.0920 - val_mae: 0.2545\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0727 - mae: 0.2348 - val_loss: 0.0924 - val_mae: 0.2531\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0726 - mae: 0.2342 - val_loss: 0.0934 - val_mae: 0.2519\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0731 - mae: 0.2345 - val_loss: 0.0945 - val_mae: 0.2506\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0733 - mae: 0.2347 - val_loss: 0.0939 - val_mae: 0.2510\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0727 - mae: 0.2335 - val_loss: 0.0928 - val_mae: 0.2522\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0727 - mae: 0.2353 - val_loss: 0.0922 - val_mae: 0.2536\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0717 - mae: 0.2334 - val_loss: 0.0923 - val_mae: 0.2527\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0720 - mae: 0.2332 - val_loss: 0.0925 - val_mae: 0.2533\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0715 - mae: 0.2327 - val_loss: 0.0923 - val_mae: 0.2565\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0718 - mae: 0.2335 - val_loss: 0.0925 - val_mae: 0.2562\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0714 - mae: 0.2318 - val_loss: 0.0940 - val_mae: 0.2534\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0715 - mae: 0.2310 - val_loss: 0.0942 - val_mae: 0.2546\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0714 - mae: 0.2316 - val_loss: 0.0929 - val_mae: 0.2556\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0709 - mae: 0.2313 - val_loss: 0.0924 - val_mae: 0.2550\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0706 - mae: 0.2307 - val_loss: 0.0923 - val_mae: 0.2534\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0923 - mae: 0.2534\n",
      "Mean Absolute Error on Test Data: 0.25339534878730774\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21052\\3247992785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean Absolute Error on Test Data:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Validation Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_data' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=np.random.rand(100,10)\n",
    "y=np.random.rand(100,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1)  # Single output neuron for regression\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Absolute Error on Test Data:\", test_mae)\n",
    "predictions = model.predict(X)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\DAS27\\\\Downloads\\\\diabetes-data.csv\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_X = StandardScaler()\n",
    "\n",
    "X = scale_X.fit_transform(df1.drop([\"Outcome\"],axis = 1),)\n",
    "X = pd.DataFrame(X,columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n",
    "\n",
    "X.head()\n",
    "\n",
    "\n",
    "# Note: It is always advisable to bring all the attributes at the same scale for models such as KNN. The attributes or features with greater range will overshadow or diminish the smaller attributes/feature completely. Hence it will impact the performance of the model because it will give higher weightage to attributes with higher \n",
    "# magnitude.\n",
    "\n",
    "\n",
    "\n",
    "y=df1['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "predictions = model.predict(X_new_data)\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix (for multi-class classification)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Convolutional Neural Network (CNN), each layer serves a specific purpose in the process of \n",
    "# learning and extracting features from input data, such as images. Here's an explanation of each\n",
    "# commonly used layer type in a CNN:\n",
    "\n",
    "# Input Layer:\n",
    "\n",
    "# The input layer represents the raw input data, such as an image.\n",
    "# It specifies the shape of the input data, including its dimensions (e.g., width, height, and number \n",
    "# of color channels).\n",
    "# Convolutional Layer (Conv2D):\n",
    "\n",
    "# Convolutional layers are responsible for learning spatial hierarchies of features in the input data.\n",
    "# They slide small square-shaped filters (kernels) over the input data to perform element-wise \n",
    "# multiplication and aggregation.\n",
    "# The result is a set of feature maps that represent learned features in the input.\n",
    "# Typically, multiple convolutional layers are stacked to capture increasingly complex features.\n",
    "# Activation Function (e.g., ReLU):\n",
    "\n",
    "# After convolution, an activation function like ReLU (Rectified Linear Unit) is applied element-wise\n",
    "# to the feature maps.\n",
    "# ReLU introduces nonlinearity into the model, allowing it to learn complex patterns.\n",
    "# The most common activation function is ReLU, which replaces negative values with zeros.\n",
    "\n",
    "# Pooling Layer (MaxPooling2D or AveragePooling2D):\n",
    "# Pooling layers are used to reduce the spatial dimensions of the feature maps.\n",
    "# They help reduce computation and overfitting by retaining the most important information.\n",
    "# Max pooling selects the maximum value in each pooling region, while average pooling computes the \n",
    "# average.\n",
    "# Pooling is applied independently to each feature map.\n",
    "# Batch Normalization Layer:\n",
    "\n",
    "# Batch normalization normalizes the activations of each layer in the network.\n",
    "# It helps stabilize and accelerate training by mitigating the vanishing/exploding gradient problem.\n",
    "# Batch normalization can be applied before or after the activation function.\n",
    "\n",
    "# Dropout Layer:\n",
    "# Dropout layers randomly deactivate a fraction of neurons during training.\n",
    "# They help prevent overfitting by introducing regularization.\n",
    "# Dropout is typically applied after convolutional and fully connected layers.\n",
    "# Flatten Layer:\n",
    "\n",
    "# Before transitioning from convolutional layers to fully connected layers, a flatten layer is used.\n",
    "# It converts the 2D feature maps into a 1D vector, which can be fed into a dense layer.\n",
    "# Dense Layer (Fully Connected Layer):\n",
    "\n",
    "# Dense layers are traditional feedforward neural network layers.\n",
    "# They capture complex relationships between features and are used for classification or regression.\n",
    "# The output layer often has as many neurons as there are classes (for classification).\n",
    "# Output Layer:\n",
    "\n",
    "# The output layer produces the final prediction or classification.\n",
    "# It may use different activation functions depending on the task (e.g., softmax for classification).\n",
    "# The number of neurons in the output layer corresponds to the number of classes or regression targets.\n",
    "# These layers work together to create a hierarchical representation of the input data, with early \n",
    "# layers learning low-level  features (e.g., edges and textures) and later layers capturing higher-\n",
    "# level features and semantic information. CNNs are especially effective in tasks like image \n",
    "# classification, object detection, and image segmentation. The specific architecture\n",
    "# and hyperparameters can vary based on the problem and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f826aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 499s 3us/step\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 59s 36ms/step - loss: 1.5118 - accuracy: 0.4459 - val_loss: 1.3408 - val_accuracy: 0.5250\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 1.1832 - accuracy: 0.5801 - val_loss: 1.0913 - val_accuracy: 0.6119\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 1.0275 - accuracy: 0.6385 - val_loss: 1.0199 - val_accuracy: 0.6410\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 0.9265 - accuracy: 0.6750 - val_loss: 0.9393 - val_accuracy: 0.6739\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 52s 33ms/step - loss: 0.8560 - accuracy: 0.6984 - val_loss: 0.9534 - val_accuracy: 0.6693\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.7933 - accuracy: 0.7225 - val_loss: 0.9224 - val_accuracy: 0.6826\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 53s 34ms/step - loss: 0.7481 - accuracy: 0.7368 - val_loss: 0.8666 - val_accuracy: 0.7041\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 49s 32ms/step - loss: 0.6996 - accuracy: 0.7538 - val_loss: 0.9170 - val_accuracy: 0.6932\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 0.6647 - accuracy: 0.7654 - val_loss: 0.8614 - val_accuracy: 0.7138\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 50s 32ms/step - loss: 0.6245 - accuracy: 0.7812 - val_loss: 0.8719 - val_accuracy: 0.7113\n",
      "313/313 - 1664s - loss: 0.8719 - accuracy: 0.7113 - 1664s/epoch - 5s/step\n",
      "\n",
      "Test accuracy: 0.7113000154495239\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),   #it is used to extract the features of the image\n",
    "    layers.MaxPooling2D((2, 2)),  #it is used to reduce the spatial dimension of the data\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),  #they help to reduce the computation and overfitting\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten() #it is used to convert the layers in 1D\n",
    "    layers.Dense(64, activation='relu'),   #this layer used to predict the final output\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "# Plot training history (optional)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ec79d",
   "metadata": {},
   "source": [
    "# Generating Music using Sequence Modeling with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b443fbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 102s 5s/step - loss: 3.5544\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 94s 5s/step - loss: 3.2109\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 92s 5s/step - loss: 2.6746\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 93s 5s/step - loss: 2.1617\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 92s 5s/step - loss: 1.7968\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected shape=(64, None, 256), found shape=(1, 26, 256)\n\nCall arguments received by layer 'sequential' (type Sequential):\n   inputs=tf.Tensor(shape=(1, 26), dtype=int32)\n   training=None\n   mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22048\\2355326546.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# Generate music\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[0mgenerated_music\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X:1\\nT:Generated Music\\nK:C\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_music\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22048\\2355326546.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[1;34m(model, start_string, temperature, num_generate)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                         raise ValueError(\n\u001b[0m\u001b[0;32m    296\u001b[0m                             \u001b[1;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                             \u001b[1;34m\"incompatible with the layer: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected shape=(64, None, 256), found shape=(1, 26, 256)\n\nCall arguments received by layer 'sequential' (type Sequential):\n   inputs=tf.Tensor(shape=(1, 26), dtype=int32)\n   training=None\n   mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the Nottingham dataset\n",
    "nottingham_dir = \"C:\\\\Users\\\\DAS27\\\\Downloads\\\\archive (26)\"\n",
    "with open(os.path.join(nottingham_dir, 'input_revised.txt'), 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "vocab = sorted(set(data))\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convert the data to integer representations\n",
    "data_as_int = np.array([char2idx[c] for c in data])\n",
    "\n",
    "# Define a function to create training examples and targets\n",
    "seq_length = 100  # Length of the input sequence for each training example\n",
    "examples_per_epoch = len(data) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(data_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "# Build the model\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Define a function to generate text using the trained model\n",
    "def generate_text(model, start_string, temperature=1.0, num_generate=1000):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 5\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "# Generate music\n",
    "generated_music = generate_text(model, start_string=\"X:1\\nT:Generated Music\\nK:C\\n\", num_generate=1000)\n",
    "print(generated_music)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Sequence Modeling:-\n",
    "# Deep Sequence Modeling refers to the application of deep learning techniques, particularly deep neural networks, for tasks that \n",
    "# involve sequential data. Sequential data consists of data points that are ordered and have some form of temporal or sequential \n",
    "# relationship. Examples of sequential data include time series, natural language text, speech signals, DNA sequences, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks (RNNs): RNNs are a class of neural networks specifically designed for handling sequential data. They \n",
    "# have recurrent connections that allow information to flow from one time step to the next. RNNs can capture temporal dependencies\n",
    "# in data and are used in tasks like time series forecasting, natural language processing, and speech recognition.\n",
    "\n",
    "# Long Short-Term Memory (LSTM) Networks: LSTMs are a type of RNN that address the vanishing gradient problem. They have memory \n",
    "# cells that can capture long-range dependencies in sequences, making them effective for tasks where the context of previous \n",
    "# time steps is essential.\n",
    "\n",
    "# Gated Recurrent Unit (GRU) Networks: GRUs are another type of RNN architecture that is computationally more efficient than \n",
    "# LSTMs. They use gating mechanisms to control the flow of information and are suitable for tasks like text generation and \n",
    "# machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and \n",
    "# Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and \n",
    "# interpret human's languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is nlp?\n",
    "# nlp stands for natural language processing\n",
    "# speech recognition->NLU->NLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. \n",
    "# It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlu:-\n",
    "# lexical ambiguity\n",
    "# syntactic\n",
    "# semantic ambiguity \n",
    "# pragmetic ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c39dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Ambiguity: Lexical ambiguity, also known as word ambiguity, occurs when a word has multiple meanings or senses.\n",
    "# This ambiguity arises because many words in a language can have different meanings depending on the context in which they \n",
    "# are used. For example, the word \"bank\" can refer to a financial institution or the side of a river.\n",
    "\n",
    "# Syntactic Ambiguity: Syntactic ambiguity occurs when the structure or arrangement of words in a sentence allows for \n",
    "# multiple possible interpretations. This ambiguity arises due to the sentence's grammar or syntax rather than the \n",
    "# specific meanings of individual words. For example, the sentence \"saw the man with the telescope\" can be interpreted \n",
    "# in two ways: either the speaker used a telescope to see the man, or the man had the telescope with him.\n",
    "\n",
    "# Semantic Ambiguity: Semantic ambiguity arises when a sentence or phrase is unclear or has multiple possible\n",
    "# interpretations because of the meanings of the words involved. This type of ambiguity is closely related to lexical \n",
    "# ambiguity but focuses on the overall meaning of a sentence or phrase. For example, the phrase \"She saw the painting of \n",
    "# the man with the binoculars\" is semantically ambiguous because it's unclear whether the man in the painting is using\n",
    "# binoculars or if the observer is using binoculars to view the painting.\n",
    "\n",
    "# Pragmatic Ambiguity: Pragmatic ambiguity occurs when the meaning of a sentence is ambiguous due to the context,\n",
    "# background knowledge, or implied information. This type of ambiguity is related to how language is used in \n",
    "# real-world situations and how people interpret communication based on their understanding of the context. \n",
    "# Pragmatic ambiguity is often resolved through common-sense reasoning and shared knowledge. For example, if someone says,\n",
    "# \"Do you have the time?\" the meaning can be ambiguous without context; it could be a request for the current time or an\n",
    "# inquiry about one's schedule. The ambiguity is resolved based on the context and the speaker's intentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31823a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexical ambiguity:-\n",
    "# 1. The tank was full of water:-  tank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#syntactic ambiguity :-old men and women were taken to safe place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic ambiguity:-The car hit the pole while it was moving ,confusion is their"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pragmetic ambiguity:-the police are coming, kaha aa rhi h kiske liye aa rhi h kyu aa rhi h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLG:-WHAT SHOULD WE SAY TO USER?\n",
    "# it should be intelligent and conversational \n",
    "# deal with structured data\n",
    "# text/sentence planning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb64a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series:-\n",
    "# level:- average of data\n",
    "# trend\n",
    "# seasonality\n",
    "# noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series analysis is a specific way of analyzing a sequence of data points collected over a particular interval of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data is dependent on time then it is called time series\n",
    "# trend:- if increase :- uptrend\n",
    "# if decrease :- downtrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00488555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonality:- if data is repeated after one interval of time i.e. one year.\n",
    "# cyclic:- after a large duration of time , a data is repeated\n",
    "#noise:- if you are not able to find the trends and patterns from the data, then the data is noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d031fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is computer vision?\n",
    "# Computer vision is a field of artificial intelligence (AI) that enables computers and systems to derive meaningful \n",
    "# information from digital images, videos and other visual inputs  and take actions or make recommendations based on that\n",
    "# information. If AI enables computers to think, computer vision enables them to see, observe and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a20f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of \n",
    "# lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving and whether there\n",
    "# is something wrong in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a25bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big data refers to extremely large and complex datasets that are too large to be processed, managed, and analyzed using \n",
    "# traditional data processing tools and methods. The concept of big data is characterized by the \"3Vs,\" which are volume, \n",
    "# velocity, and variety, and has since expanded to include other attributes such as veracity and value:\n",
    "\n",
    "# Volume: Big data typically involves vast amounts of data. This could be terabytes, petabytes, or even exabytes of data. \n",
    "# Traditional databases and storage systems are often insufficient for handling such massive datasets.\n",
    "\n",
    "# Velocity: Data is generated and collected at an unprecedented speed in the age of big data. This includes real-time data \n",
    "# streams from sources like sensors, social media, and transaction logs. The ability to process and make decisions based on\n",
    "# data in real-time or near real-time is crucial.\n",
    "\n",
    "# Variety: Big data encompasses a wide variety of data types and formats. This includes structured data\n",
    "# (e.g., relational databases), semi-structured data (e.g., XML or JSON), unstructured data (e.g., text documents,\n",
    "# social media posts, videos, and images), and more. Big data tools and technologies are designed to handle this diversity.\n",
    "\n",
    "# Veracity: Veracity refers to the reliability and accuracy of the data. Big data often includes noisy or incomplete data, and \n",
    "# dealing with data quality and trustworthiness is a challenge.\n",
    "\n",
    "# Value: Extracting actionable insights and value from big data is the ultimate goal. Organizations aim to turn the vast amount of data they collect into valuable insights, predictions, and \n",
    "# decision-making tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f5321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
